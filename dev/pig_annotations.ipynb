{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking over annotations ------------------------------------------------------------------------------------\n",
    "# In pig 5 the microtrend failed to work.  Shortly before that point the resuscitation algorithm had changed so that \n",
    "    # it wasn't needed but there was still code in it that expected values to be there.  So the resuscitation code was\n",
    "    # crashing on that.  I needed to remove the code that depended on the pCO2 values and then restart the system.  \n",
    "    # Thus there are two sessions, one that leads up to the resuscitation and one that goes from when the resuscitation starts.\n",
    "\n",
    "# for pig in ['pig_06_20200922_084158']: # additional annotation: Irvin Pierskalla collected PCO2 data from three devices during yesterday's \n",
    "    # pig study.  The data from our linked device was useless. However, the CVS file attached from one of the two other\n",
    "    # probes gave excellent trending data. The first 45 minutes is actually human data, Irvin but the probe in his \n",
    "    # own mouth to make sure it was accurate! Then we switched it into the pig. I believe the time is one hour off as \n",
    "    # he was using Central Time. Still, the change sin PCO2 trend nicely baseline, bleed, wait and resuscitation phases.       Please add this file to the #6 pig data file and delete the PCO2 one already there or mark it \"bad data\".\n",
    "\n",
    "# for pig in ['pig_07_20200924_084150']: # just a formatting issue with \\'\n",
    "\n",
    "# for pig in ['pig_08_20201013_080142']: # just differnt spacing\n",
    "\n",
    "# for pig in ['pig_13_03252021_501_20210325_082946']: # just \" instead of ' \n",
    "\n",
    "# for pig in ['pig_14_20210330_113521']: # just a formatting issue with \\'\n",
    "\n",
    "# In pig 16, the device that records the timing synchronization data for the Braedius camera/laptop wasn't working.  \n",
    "    # In the supplemental session, I have that \"working\".  (I say \"working\" because I just found out that I wasn't \n",
    "    # sending all of the synchronization time bits and it now becomes a puzzle to decode the times stored in \n",
    "    # \"daq.Clicktrack\") The supplemental session starts only after resuscitation has completely finished. Many of \n",
    "    # the devices were already shutdown by the time I created a new session and the session only lasts for\n",
    "    # about 12 minutes or so\n",
    "\n",
    "# 1st 12 pigs getting norep delayed first .5ml, after fixed\n",
    "# 1,2,3 came close to failure\n",
    "# dobutamine was available but perhaps not used in pigs 1-14\n",
    "\n",
    "import os\n",
    "\n",
    "path = '/zfsauton/data/public/vleonard/tracir/'\n",
    "\n",
    "for pig in sorted(os.listdir(path+'sessions/')):\n",
    "\n",
    "    if pig in ['pig_04_20200908_101618', 'pig_05_20200910_095434', 'pig_16_20210518_184343_supplemental', 'pig_24_20211130_125421']: continue\n",
    "    print(pig)\n",
    "    print('processing file:', path+'sessions/'+pig+'/logs/logfile_debug.log')\n",
    "    file1 = open(path+'sessions/'+pig+'/logs/logfile_debug.log', 'r')\n",
    "    lines = file1.readlines()\n",
    "    \n",
    "    annotations1 = [l for l in lines if 'Adding annotation' in l]\n",
    "    \n",
    "    if os.path.exists(path+'sessions/'+pig+'/annotations.txt'):\n",
    "        file1 = open(path+'sessions/'+pig+'/annotations.txt', 'r')\n",
    "    else:\n",
    "        print(path+'sessions/'+pig+'/annotations.txt does not exist')\n",
    "        continue    \n",
    "    lines = file1.readlines()\n",
    "    annotations2 = [l for l in lines if 'text' in l]\n",
    "    annotations2 = [l.replace(\"\\\"\", \"\\'\").replace('    ','').replace('\\n','') for l in annotations2]\n",
    "#     for a in annotations1:\n",
    "#         print(a)\n",
    "#     for a in annotations2:\n",
    "#         print(a)\n",
    "        \n",
    "    print('lengths check:', len(annotations1)==len(annotations2), len(annotations1), len(annotations2))\n",
    "    for i in range(len(annotations1)):\n",
    "        # print(annotations2[i] in annotations1[i])\n",
    "        if annotations2[i] not in annotations1[i]:\n",
    "            print('mismatch found:')\n",
    "            print(annotations2[i])\n",
    "            print(annotations1[i])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bba3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking over annotations ------------------------------------------------------------------------------------\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# path = '/zfsauton/data/public/xinyul2/PigBleed/2020/'\n",
    "path = '/zfsauton/data/public/vleonard/tracir/'\n",
    "\n",
    "pig_nums = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', \n",
    "            '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\n",
    "\n",
    "pig_logs = {}\n",
    "\n",
    "for num in pig_nums:\n",
    "    paths = sorted(glob.glob(path+\"sessions/pig_\"+num+\"*\"))    \n",
    "    print('pig', num, paths)\n",
    "    pig_logs[num] = []\n",
    "    for p in paths:\n",
    "        file1 = open(p+'/logs/logfile_debug.log', 'r')\n",
    "        lines = file1.readlines()\n",
    "        pig_logs[num].extend(lines)\n",
    "\n",
    "        print('len of logs', len(pig_logs[num]))\n",
    "    \n",
    "    err1 = [l for l in pig_logs[num] if ('ERROR' in l ) and (('devices.pumps.neurowave' in l) or ('devices.neurowave.pump' in l))]\n",
    "    if len(err1) > 0:\n",
    "        print('Example Error', err1[0])\n",
    "    # time = [' '.join(l.split(' ')[1:3]) for l in pig_logs[num]]\n",
    "\n",
    "# for pig in ['pig_05_20200910_095434', 'pig_05_20200910_150930']:\n",
    "# for pig in ['pig_16_20210518_184343_supplemental', 'pig_16_20210518_081155']:\n",
    "#     print('processing file:', path+'sessions/'+pig+'/logs/logfile_debug.log')\n",
    "#     file1 = open(path+'sessions/'+pig+'/logs/logfile_debug.log', 'r')\n",
    "#     lines = file1.readlines()\n",
    "    \n",
    "#     annotations1 = [l for l in lines if 'Adding annotation' in l]\n",
    "        \n",
    "# #     file1 = open(path+'sessions/'+pig+'/annotations.txt', 'r')\n",
    "# #     lines = file1.readlines()\n",
    "# #     annotations2 = [l for l in lines if 'text' in l]\n",
    "# #     annotations2 = [l.replace(\"\\\"\", \"\\'\").replace('    ','').replace('\\n','') for l in annotations2]\n",
    "#     for a in annotations1:\n",
    "#         print(a)\n",
    "# #     for a in annotations2:\n",
    "# #         print(a)\n",
    "        \n",
    "# #     print(len(annotations1)==len(annotations2), len(annotations1), len(annotations2))\n",
    "# #     for i in range(len(annotations1)):\n",
    "# #         print(annotations2[i] in annotations1[i])\n",
    "# #         if annotations2[i] not in annotations1[i]:\n",
    "# #             print(annotations2[i])\n",
    "# #             print(annotations1[i])\n",
    "# # #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7971a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plotting the derivative of timestamps \n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "h5_paths = \"/zfsauton/data/public/vleonard/tracir/auv_files/\"\n",
    "paths = os.listdir(h5_paths)\n",
    "paths.remove('p06_microtrend_data.xlsx')\n",
    "paths = sorted(paths)\n",
    "print(paths)\n",
    "\n",
    "path_dict = {path: {} for path in paths}\n",
    "# [1,2,3,4,7,8]\n",
    "# [2,3,4,7,8] - [1,2,3,4,7]\n",
    "\n",
    "for path in paths:\n",
    "    f = h5py.File(h5_paths+path, 'r')\n",
    "    \n",
    "    plt.figure(1, figsize=(15,5))\n",
    "    if 'pump.fluid' in f.keys():\n",
    "        pump_fluid = pd.DataFrame(f['pump.fluid'][()])\n",
    "        print(pump_fluid.columns, pump_fluid.dtypes)\n",
    "        path_dict[path]['pump.fluid'] = pump_fluid.timestamp.values[1:]-pump_fluid.timestamp.values[:-1]\n",
    "\n",
    "        plt.plot(pump_fluid.timestamp.values[1:]-pump_fluid.timestamp.values[:-1], label='pump.fluid')\n",
    "\n",
    "    if 'pump.dobut' in f.keys():\n",
    "        pump_dobut = pd.DataFrame(f['pump.dobut'][()])\n",
    "        # print(pump_dobut.columns, pump_dobut.dtypes)\n",
    "        path_dict[path]['pump.dobut'] = pump_dobut.timestamp.values[1:]-pump_dobut.timestamp.values[:-1]\n",
    "\n",
    "        plt.plot(pump_dobut.timestamp.values[1:]-pump_dobut.timestamp.values[:-1], label='pump.dobut')\n",
    "        \n",
    "    plt.legend(loc='upper left')        \n",
    "    plt.title(path + ' pump time derivatives'); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# pump_fluid.timestamp.values[0]\n",
    "timestamp = 1639660632.315818\n",
    "dt_object = datetime.fromtimestamp(timestamp)\n",
    "print(dt_object)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/zfsauton/data/public/xinyul2/PigBleed/2020/waveform/'\n",
    "\n",
    "for filename in sorted(os.listdir(path)):\n",
    "    data = pd.read_csv(path+filename)\n",
    "\n",
    "    print(filename, list(data.columns), data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1134292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1504, 130)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('p01.npy')\n",
    "data = data.squeeze()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f67545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN  # kNN detector\n",
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.data import generate_data\n",
    "\n",
    "X, y= generate_data(train_only=True)  # load data\n",
    "\n",
    "# initialize 20 base detectors for combination\n",
    "k_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140,\n",
    "            150, 160, 170, 180, 190, 200]\n",
    "n_clf = len(k_list) # Number of classifiers being trained\n",
    "\n",
    "train_scores = np.zeros([X_train.shape[0], n_clf])\n",
    "test_scores = np.zeros([X_test.shape[0], n_clf])\n",
    "\n",
    "for i in range(n_clf):\n",
    "    k = k_list[i]\n",
    "\n",
    "    clf = KNN(n_neighbors=k, method='largest')\n",
    "    clf.fit(X_train_norm)\n",
    "\n",
    "    train_scores[:, i] = clf.decision_scores_\n",
    "    test_scores[:, i] = clf.decision_function(X_test_norm)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
