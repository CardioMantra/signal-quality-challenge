{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking over annotations ------------------------------------------------------------------------------------\n",
    "# In pig 5 the microtrend failed to work.  Shortly before that point the resuscitation algorithm had changed so that \n",
    "    # it wasn't needed but there was still code in it that expected values to be there.  So the resuscitation code was\n",
    "    # crashing on that.  I needed to remove the code that depended on the pCO2 values and then restart the system.  \n",
    "    # Thus there are two sessions, one that leads up to the resuscitation and one that goes from when the resuscitation starts.\n",
    "\n",
    "# for pig in ['pig_06_20200922_084158']: # additional annotation: Irvin Pierskalla collected PCO2 data from three devices during yesterday's \n",
    "    # pig study.  The data from our linked device was useless. However, the CVS file attached from one of the two other\n",
    "    # probes gave excellent trending data. The first 45 minutes is actually human data, Irvin but the probe in his \n",
    "    # own mouth to make sure it was accurate! Then we switched it into the pig. I believe the time is one hour off as \n",
    "    # he was using Central Time. Still, the change sin PCO2 trend nicely baseline, bleed, wait and resuscitation phases.       Please add this file to the #6 pig data file and delete the PCO2 one already there or mark it \"bad data\".\n",
    "\n",
    "# for pig in ['pig_07_20200924_084150']: # just a formatting issue with \\'\n",
    "\n",
    "# for pig in ['pig_08_20201013_080142']: # just differnt spacing\n",
    "\n",
    "# for pig in ['pig_13_03252021_501_20210325_082946']: # just \" instead of ' \n",
    "\n",
    "# for pig in ['pig_14_20210330_113521']: # just a formatting issue with \\'\n",
    "\n",
    "# In pig 16, the device that records the timing synchronization data for the Braedius camera/laptop wasn't working.  \n",
    "    # In the supplemental session, I have that \"working\".  (I say \"working\" because I just found out that I wasn't \n",
    "    # sending all of the synchronization time bits and it now becomes a puzzle to decode the times stored in \n",
    "    # \"daq.Clicktrack\") The supplemental session starts only after resuscitation has completely finished. Many of \n",
    "    # the devices were already shutdown by the time I created a new session and the session only lasts for\n",
    "    # about 12 minutes or so\n",
    "\n",
    "# 1st 12 pigs getting norep delayed first .5ml, after fixed\n",
    "# 1,2,3 came close to failure\n",
    "# dobutamine was available but perhaps not used in pigs 1-14\n",
    "\n",
    "import os\n",
    "\n",
    "path = '/zfsauton/data/public/vleonard/tracir/'\n",
    "\n",
    "for pig in sorted(os.listdir(path+'sessions/')):\n",
    "\n",
    "    if pig in ['pig_04_20200908_101618', 'pig_05_20200910_095434', 'pig_16_20210518_184343_supplemental', 'pig_24_20211130_125421']: continue\n",
    "    print(pig)\n",
    "    print('processing file:', path+'sessions/'+pig+'/logs/logfile_debug.log')\n",
    "    file1 = open(path+'sessions/'+pig+'/logs/logfile_debug.log', 'r')\n",
    "    lines = file1.readlines()\n",
    "    \n",
    "    annotations1 = [l for l in lines if 'Adding annotation' in l]\n",
    "    \n",
    "    if os.path.exists(path+'sessions/'+pig+'/annotations.txt'):\n",
    "        file1 = open(path+'sessions/'+pig+'/annotations.txt', 'r')\n",
    "    else:\n",
    "        print(path+'sessions/'+pig+'/annotations.txt does not exist')\n",
    "        continue    \n",
    "    lines = file1.readlines()\n",
    "    annotations2 = [l for l in lines if 'text' in l]\n",
    "    annotations2 = [l.replace(\"\\\"\", \"\\'\").replace('    ','').replace('\\n','') for l in annotations2]\n",
    "#     for a in annotations1:\n",
    "#         print(a)\n",
    "#     for a in annotations2:\n",
    "#         print(a)\n",
    "        \n",
    "    print('lengths check:', len(annotations1)==len(annotations2), len(annotations1), len(annotations2))\n",
    "    for i in range(len(annotations1)):\n",
    "        # print(annotations2[i] in annotations1[i])\n",
    "        if annotations2[i] not in annotations1[i]:\n",
    "            print('mismatch found:')\n",
    "            print(annotations2[i])\n",
    "            print(annotations1[i])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bba3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# looking over annotations ------------------------------------------------------------------------------------\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# path = '/zfsauton/data/public/xinyul2/PigBleed/2020/'\n",
    "path = '/zfsauton/data/public/vleonard/tracir/'\n",
    "\n",
    "pig_nums = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', \n",
    "            '11', '12', '13', '14', '15', '16', '17', '18', '19', '20']\n",
    "\n",
    "pig_logs = {}\n",
    "\n",
    "for num in pig_nums:\n",
    "    paths = sorted(glob.glob(path+\"sessions/pig_\"+num+\"*\"))    \n",
    "    print('pig', num, paths)\n",
    "    pig_logs[num] = []\n",
    "    for p in paths:\n",
    "        file1 = open(p+'/logs/logfile_debug.log', 'r')\n",
    "        lines = file1.readlines()\n",
    "        pig_logs[num].extend(lines)\n",
    "\n",
    "        print('len of logs', len(pig_logs[num]))\n",
    "    \n",
    "    err1 = [l for l in pig_logs[num] if ('ERROR' in l ) and (('devices.pumps.neurowave' in l) or ('devices.neurowave.pump' in l))]\n",
    "    if len(err1) > 0:\n",
    "        print('Example Error', err1[0])\n",
    "    # time = [' '.join(l.split(' ')[1:3]) for l in pig_logs[num]]\n",
    "\n",
    "# for pig in ['pig_05_20200910_095434', 'pig_05_20200910_150930']:\n",
    "# for pig in ['pig_16_20210518_184343_supplemental', 'pig_16_20210518_081155']:\n",
    "#     print('processing file:', path+'sessions/'+pig+'/logs/logfile_debug.log')\n",
    "#     file1 = open(path+'sessions/'+pig+'/logs/logfile_debug.log', 'r')\n",
    "#     lines = file1.readlines()\n",
    "    \n",
    "#     annotations1 = [l for l in lines if 'Adding annotation' in l]\n",
    "        \n",
    "# #     file1 = open(path+'sessions/'+pig+'/annotations.txt', 'r')\n",
    "# #     lines = file1.readlines()\n",
    "# #     annotations2 = [l for l in lines if 'text' in l]\n",
    "# #     annotations2 = [l.replace(\"\\\"\", \"\\'\").replace('    ','').replace('\\n','') for l in annotations2]\n",
    "#     for a in annotations1:\n",
    "#         print(a)\n",
    "# #     for a in annotations2:\n",
    "# #         print(a)\n",
    "        \n",
    "# #     print(len(annotations1)==len(annotations2), len(annotations1), len(annotations2))\n",
    "# #     for i in range(len(annotations1)):\n",
    "# #         print(annotations2[i] in annotations1[i])\n",
    "# #         if annotations2[i] not in annotations1[i]:\n",
    "# #             print(annotations2[i])\n",
    "# #             print(annotations1[i])\n",
    "# # #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7971a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plotting the derivative of timestamps \n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "h5_paths = \"/zfsauton/data/public/vleonard/tracir/auv_files/\"\n",
    "paths = os.listdir(h5_paths)\n",
    "paths.remove('p06_microtrend_data.xlsx')\n",
    "paths = sorted(paths)\n",
    "print(paths)\n",
    "\n",
    "path_dict = {path: {} for path in paths}\n",
    "# [1,2,3,4,7,8]\n",
    "# [2,3,4,7,8] - [1,2,3,4,7]\n",
    "\n",
    "for path in paths:\n",
    "    f = h5py.File(h5_paths+path, 'r')\n",
    "    \n",
    "    plt.figure(1, figsize=(15,5))\n",
    "    if 'pump.fluid' in f.keys():\n",
    "        pump_fluid = pd.DataFrame(f['pump.fluid'][()])\n",
    "        print(pump_fluid.columns, pump_fluid.dtypes)\n",
    "        path_dict[path]['pump.fluid'] = pump_fluid.timestamp.values[1:]-pump_fluid.timestamp.values[:-1]\n",
    "\n",
    "        plt.plot(pump_fluid.timestamp.values[1:]-pump_fluid.timestamp.values[:-1], label='pump.fluid')\n",
    "\n",
    "    if 'pump.dobut' in f.keys():\n",
    "        pump_dobut = pd.DataFrame(f['pump.dobut'][()])\n",
    "        # print(pump_dobut.columns, pump_dobut.dtypes)\n",
    "        path_dict[path]['pump.dobut'] = pump_dobut.timestamp.values[1:]-pump_dobut.timestamp.values[:-1]\n",
    "\n",
    "        plt.plot(pump_dobut.timestamp.values[1:]-pump_dobut.timestamp.values[:-1], label='pump.dobut')\n",
    "        \n",
    "    plt.legend(loc='upper left')        \n",
    "    plt.title(path + ' pump time derivatives'); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# pump_fluid.timestamp.values[0]\n",
    "timestamp = 1639660632.315818\n",
    "dt_object = datetime.fromtimestamp(timestamp)\n",
    "print(dt_object)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/zfsauton/data/public/xinyul2/PigBleed/2020/waveform/'\n",
    "\n",
    "for filename in sorted(os.listdir(path)):\n",
    "    data = pd.read_csv(path+filename)\n",
    "\n",
    "    print(filename, list(data.columns), data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1134292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['p01.h5', 'p02.h5', 'p03.h5', 'p04.h5', 'p05.h5', 'p06.h5', 'p07.h5', 'p08.h5', 'p09.h5', 'p10.h5', 'p11.h5', 'p12.h5', 'p13.h5', 'p14.h5', 'p15.h5', 'p16.h5', 'p17.h5', 'p18.h5', 'p19.h5', 'p20.h5', 'p21.h5', 'p22.h5', 'p23.h5', 'p24.h5', 'p25.h5', 'p26.h5']\n",
      "p01.h5 Index(['timestamp', 'CCO', 'SvO2', 'Pleth', 'SpO2', 'PAP', 'CVP', 'ART',\n",
      "       'AirPr', 'ECG'],\n",
      "      dtype='object') (3759700, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1504 [00:00<17:02,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1504 [00:01<12:05,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1504 [00:01<10:50,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from datetime import datetime\n",
    "import neurokit2 as nk\n",
    "from tqdm import trange\n",
    "sys.path.append('../')\n",
    "from signal_quality import featurization, sqis\n",
    "\n",
    "\n",
    "h5_paths = \"/zfsauton/data/public/vleonard/tracir/auv_files/\"\n",
    "paths = os.listdir(h5_paths)\n",
    "paths.remove('p06_microtrend_data.xlsx')\n",
    "paths = sorted(paths)\n",
    "print(paths)\n",
    "\n",
    "path_dict = {path: {} for path in paths}\n",
    "# [1,2,3,4,7,8]\n",
    "# [2,3,4,7,8] - [1,2,3,4,7]\n",
    "\n",
    "def plot_df(data, title):\n",
    "    fig, axs = plt.subplots(nrows=len(data.columns), figsize=(15,15))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(len(axs)):\n",
    "        ylims = (np.percentile(data.iloc[:,i].values, q=1), np.percentile(data.iloc[:,i].values, q=99))\n",
    "        \n",
    "        axs[i].plot(data.index, data.iloc[:,i], alpha=.5) \n",
    "        axs[i].set_ylim(ylims[0], ylims[1])\n",
    "        axs[i].set_ylabel(data.columns[i])\n",
    "\n",
    "        # axs[i].legend(loc='lower left'); \n",
    "        axs[i].grid()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "sampling_rate = 125\n",
    "step_size = 10 # seconds\n",
    "for path in paths:\n",
    "    f = h5py.File(h5_paths+path, 'r')\n",
    "    data = pd.DataFrame(f['daq'][()])\n",
    "\n",
    "    for bad_col in ['ECG_SQ', 'XXXX', 'Clicktrack']:\n",
    "        if bad_col in data.columns:\n",
    "            data.drop(columns=[bad_col], inplace=True)\n",
    "\n",
    "    print(path, data.columns, data.shape)\n",
    "\n",
    "    data['timestamp'] -= np.min(data['timestamp'])\n",
    "    # data['timestamp'] /= 3600\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    # plot_df(data, title=path)\n",
    "    # data.plot(use_index=True, ylim=ylims, subplots=True, figsize=(15,15), grid=True, xlabel='hours')\n",
    "\n",
    "    start = data.index[0]\n",
    "    end = data.index[-1]\n",
    "\n",
    "    all_features = []\n",
    "    for i in trange(int(start), int(end), step_size):\n",
    "        window = data.iloc[(i<data.index) & (data.index<i+step_size)]\n",
    "\n",
    "        pleth_raw = window['Pleth'].values\n",
    "        pleth_cleaned = nk.ppg_clean(ppg_signal=pleth_raw, sampling_rate=sampling_rate, method='elgendi')\n",
    "        pleth_features = featurization.featurize_pleth(window=pleth_cleaned, pleth_time=window.index.values) + \\\n",
    "            sqis.get_pleth_sqis(pleth_raw, pleth_cleaned)\n",
    "\n",
    "        ecg_raw = window['ECG'].values\n",
    "        ecg_cleaned = nk.ecg_clean(ecg_raw, sampling_rate=sampling_rate, method=\"neurokit\")\n",
    "        peaks = nk.ecg_peaks(ecg_cleaned, sampling_rate=sampling_rate, method='kalidas2017')[1]['ECG_R_Peaks']\n",
    "        ecg_features = featurization.featurize_ecg(window=ecg_cleaned, sampling_rate=sampling_rate) + \\\n",
    "            sqis.get_ecg_sqis(ecg_raw, ecg_cleaned, peaks, sampling_rate, window=step_size)\n",
    "\n",
    "        cco_features = sqis.get_generic_sqis(window['CCO'].values)\n",
    "        svo2_features = sqis.get_generic_sqis(window['SvO2'].values)\n",
    "        spo2_features = sqis.get_generic_sqis(window['SpO2'].values)\n",
    "        pap_features = sqis.get_generic_sqis(window['PAP'].values)\n",
    "        cvp_features = sqis.get_generic_sqis(window['CVP'].values)\n",
    "        art_features = sqis.get_generic_sqis(window['ART'].values)\n",
    "        airpr_features = sqis.get_generic_sqis(window['AirPr'].values)\n",
    "\n",
    "        all_features.append(pleth_features + ecg_features + cco_features + svo2_features + spo2_features + \\\n",
    "            pap_features + cvp_features + art_features + airpr_features)\n",
    "        print(len(all_features[-1]))\n",
    "        break\n",
    "\n",
    "    np.save(file=path[:-3]+'.npy', arr=np.array(all_features))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
