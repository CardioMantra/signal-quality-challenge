{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:09:30.059064Z",
     "start_time": "2024-12-20T01:09:26.766767Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from tqdm.notebook import tqdm\n",
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import sys\n",
    "import sklearn\n",
    "import datasets, sqis, featurization, helpers"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:09:31.866025Z",
     "start_time": "2024-12-20T01:09:31.860743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_features(subject_dict, sampling_rate=125, window_size=10):\n",
    "    X_features_dict = {\n",
    "        'features': [],\n",
    "        'y_list': []\n",
    "    }\n",
    "\n",
    "    ## Calculate multi-lead features\n",
    "    ## Remove baseline wader and dc offset with highpass Butterworth. Also remove powerline interference (50hz).        \n",
    "    ecg_cleaned_list = [\n",
    "        nk.ecg_clean(subject_dict[channel]['data'], sampling_rate=sampling_rate, method=\"neurokit\")\n",
    "        for channel in subject_dict.keys()\n",
    "        ]\n",
    "\n",
    "    i_sqi = sqis.i_sqi(ecg_cleaned_list, sampling_rate)\n",
    "    pca_sqi = sqis.pca_sqi(np.array(ecg_cleaned_list).T) # 12 features\n",
    "\n",
    "    ## Calculate single-lead features\n",
    "    for i, channel in enumerate(subject_dict.keys()):\n",
    "        ecg_raw = subject_dict[channel]['data']\n",
    "        ecg_cleaned = ecg_cleaned_list[i]\n",
    "\n",
    "        ## Find peaks indices\n",
    "        peaks = nk.ecg_peaks(ecg_cleaned, sampling_rate=sampling_rate, method='kalidas2017')[1]['ECG_R_Peaks']\n",
    "\n",
    "        ## Featurize ecgs\n",
    "        ecg_features = featurization.featurize_ecg(window=ecg_cleaned, sampling_rate=sampling_rate)\n",
    "        ecg_sqis = sqis.get_ecg_sqis(ecg_raw, ecg_cleaned, peaks, sampling_rate, window=window_size) + [i_sqi, pca_sqi]\n",
    "\n",
    "        X_features_dict['y_list'].append(subject_dict[channel]['label'])\n",
    "        X_features_dict['features'].append(ecg_features + ecg_sqis)\n",
    "\n",
    "    return X_features_dict"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:11:31.588750Z",
     "start_time": "2024-12-20T01:09:36.732353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path='./data/PICC/'\n",
    "output_dict = datasets.load_picc(data_path=data_path, verbose=False)\n",
    "\n",
    "X_features_dict = {'features': [], 'y_list': [], 'subject': []}\n",
    "\n",
    "X_features_dict = helpers.generate_features_dict(output_dict, X_features_dict)\n",
    "\n",
    "# with multiprocessing.Pool(processes=10) as pool:\n",
    "#     X_features_dicts = list(tqdm(pool.imap(get_features, [output_dict[subject] for subject in output_dict.keys()]), total=len(output_dict.keys())))\n",
    "#     for i, d in enumerate(X_features_dicts):\n",
    "#         for key in d.keys():\n",
    "#             X_features_dict[key].extend(d[key])\n",
    "#         \n",
    "#         X_features_dict['subject'].extend([i for _ in range(len(d['y_list']))])\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "998it [00:02, 443.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fb0fb34ae2b49d99fe41bcdc147bc35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'zhao2018'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m output_dict \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mload_picc(data_path\u001B[38;5;241m=\u001B[39mdata_path, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      4\u001B[0m X_features_dict \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m: [], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_list\u001B[39m\u001B[38;5;124m'\u001B[39m: [], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m: []}\n\u001B[0;32m----> 6\u001B[0m X_features_dict \u001B[38;5;241m=\u001B[39m \u001B[43mhelpers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_features_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_features_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# with multiprocessing.Pool(processes=10) as pool:\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#     X_features_dicts = list(tqdm(pool.imap(get_features, [output_dict[subject] for subject in output_dict.keys()]), total=len(output_dict.keys())))\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#     for i, d in enumerate(X_features_dicts):\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m#         \u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#         X_features_dict['subject'].extend([i for _ in range(len(d['y_list']))])\u001B[39;00m\n",
      "File \u001B[0;32m~/PedroDeParkes/CardioMantra/RnD/signal-quality-challenge/helpers.py:132\u001B[0m, in \u001B[0;36mgenerate_features_dict\u001B[0;34m(output_dict, X_features_dict)\u001B[0m\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(X_features_dicts):\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;66;03m# print(f\"i: {i}, d:{d}\")\u001B[39;00m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m d\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    131\u001B[0m             \u001B[38;5;66;03m# print(f\"key: {key}\")\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m             \u001B[43mX_features_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mextend(d[key])\n\u001B[1;32m    134\u001B[0m         X_features_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mextend([i \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(d[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_list\u001B[39m\u001B[38;5;124m'\u001B[39m]))])\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X_features_dict\n",
      "\u001B[0;31mKeyError\u001B[0m: 'zhao2018'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-12-20T01:12:07.220334Z",
     "start_time": "2024-12-20T01:12:03.369645Z"
    }
   },
   "source": [
    "## Note that exact results cannot be reproduced as the official PICC challenge is trained on the entirety of set-a and evaluated on set-b\n",
    "import sklearn.ensemble\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.ecod import ECOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.pca import PCA\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys, os\n",
    "\n",
    "subjects = np.array(X_features_dict['subject'])\n",
    "gkf = sklearn.model_selection.GroupKFold(n_splits=5)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "print('Method, Train AUC, Train Accuracy, Test AUC, Test Accuracy')\n",
    "# for method in ['KNN', \"IForest\", \"PCA\", \"OCSVM\", \"AutoEncoder\"]:\n",
    "for method in ['KNN', \"IForest\", \"OCSVM\", \"AutoEncoder\"]:\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    test_accs = []\n",
    "    test_tprs = []\n",
    "    test_aucs = []\n",
    "    train_accs = []\n",
    "    train_aucs = []\n",
    "\n",
    "    for train_split, test_split in gkf.split(subjects, groups=subjects):\n",
    "        # print(\"%s %s\" % (subjects[train_split], subjects[test_split]))\n",
    "        X_train_features_cleaned = np.nan_to_num(np.array(X_features_dict['features'])[train_split], nan=0.0, posinf=10000, neginf=-10000)\n",
    "        X_test_features_cleaned = np.nan_to_num(np.array(X_features_dict['features'])[test_split], nan=0.0, posinf=10000, neginf=-10000)\n",
    "        y_train = np.array(X_features_dict['y_list'])[train_split]\n",
    "        y_test = np.array(X_features_dict['y_list'])[test_split]\n",
    "    \n",
    "        # model = sklearn.ensemble.RandomForestClassifier(random_state=0, n_estimators=1000, max_depth=5, n_jobs=10)\n",
    "        # model.fit(X_train_features_cleaned, y_train)\n",
    "        # scores = model.predict_proba(X_test_features_cleaned)[:,1]\n",
    "        contamination = np.sum(y_train==1)/len(y_train)\n",
    "        print(contamination)\n",
    "\n",
    "        if method == 'IForest':\n",
    "            model = IForest(n_estimators=1000, max_samples=\"auto\", contamination=contamination, max_features=1.0, \n",
    "                bootstrap=False, n_jobs=10, behaviour='new', random_state=0)            \n",
    "        elif method == 'KNN':\n",
    "            model = KNN(contamination=contamination, n_neighbors=5, method='median', radius=1.0, algorithm='auto', \n",
    "                leaf_size=30, metric='l1', p=2, metric_params=None, n_jobs=1)\n",
    "        elif method == 'PCA':\n",
    "            model = PCA(n_components=None, n_selected_components=None, contamination=contamination, copy=True, \n",
    "                whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None, \n",
    "                weighted=True, standardization=True)\n",
    "        elif method == 'OCSVM':\n",
    "            model =  OCSVM(kernel='rbf', degree=3, gamma='auto', coef0=0.0, tol=0.001, nu=0.5, shrinking=True, \n",
    "                cache_size=200, verbose=False, max_iter=- 1, contamination=contamination)\n",
    "        elif method == 'AutoEncoder':\n",
    "            torch.manual_seed(0)\n",
    "            np.random.seed(0)\n",
    "            model = AutoEncoder(hidden_neurons=[32, 32], hidden_activation='relu', batch_norm=True, \n",
    "                learning_rate=0.001, epochs=100, batch_size=32, dropout_rate=0.2, weight_decay=1e-05, \n",
    "                preprocessing=True, loss_fn=None, contamination=contamination, device=None)\n",
    "        \n",
    "        sys.stdout = open(os.devnull, 'w') # Block print from autoencoder fitting\n",
    "        model.fit(X_train_features_cleaned)\n",
    "        sys.stdout = sys.__stdout__ # reenable print\n",
    "\n",
    "        test_scores = model.decision_function(X_test_features_cleaned)\n",
    "        train_scores = model.decision_function(X_train_features_cleaned)\n",
    "\n",
    "        train_accs.append(sklearn.metrics.accuracy_score(y_true=y_train, y_pred=train_scores > model.threshold_))\n",
    "        train_aucs.append(sklearn.metrics.roc_auc_score(y_true=y_train, y_score=train_scores))\n",
    "        test_accs.append(sklearn.metrics.accuracy_score(y_true=y_test, y_pred=test_scores > model.threshold_))\n",
    "        test_aucs.append(sklearn.metrics.roc_auc_score(y_true=y_test, y_score=test_scores))\n",
    "\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, test_scores, pos_label=1)\n",
    "        test_tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        \n",
    "\n",
    "    mean_tpr = np.mean(test_tprs, axis=0)\n",
    "    std_tpr = np.std(test_tprs, axis=0)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(mean_fpr, mean_tpr, label=method)\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.grid(); plt.legend(loc=\"lower right\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), color=\"navy\", linestyle=\"--\")\n",
    "    plt.xscale('log'); \n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(mean_fpr, mean_tpr, label=method)\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.grid(); plt.legend(loc=\"lower right\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), color=\"navy\", linestyle=\"--\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(1-mean_tpr, 1-mean_fpr, label=method)\n",
    "    plt.xlabel('FNR'); plt.ylabel('TNR'); plt.grid(); plt.legend(loc=\"upper left\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), color=\"navy\", linestyle=\"--\")\n",
    "    plt.xscale('log')\n",
    "    \n",
    "#     plt.plot(mean_fpr, mean_tpr, label=method)\n",
    "    # plt.fill_between(mean_fpr, mean_tpr-std_tpr, mean_tpr+std_tpr, alpha=.2)\n",
    "    print(\"{}, {:0.3f} $\\pm$ {:0.3f}, {:0.3f} $\\pm$ {:0.3f} , {:0.3f} $\\pm$ {:0.3f}, {:0.3f} $\\pm$ {:0.3f}\".format(\\\n",
    "        method, np.mean(test_aucs), np.std(test_aucs), np.mean(test_accs), np.std(test_accs), \\\n",
    "        np.mean(train_aucs), np.std(train_aucs), np.mean(train_accs), np.std(train_accs)))\n",
    "plt.subplot(1, 3, 1); plt.grid();\n",
    "plt.subplot(1, 3, 2); plt.grid();\n",
    "plt.subplot(1, 3, 3); plt.grid();\n",
    "plt.show()\n",
    "# plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.0]); \n",
    "# plt.xscale('log')\n",
    "# plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\"); plt.title(\"Test ROC Curve\")\n",
    "# plt.legend(loc=\"lower right\"); plt.grid()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method, Train AUC, Train Accuracy, Test AUC, Test Accuracy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m train_accs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     26\u001B[0m train_aucs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train_split, test_split \u001B[38;5;129;01min\u001B[39;00m gkf\u001B[38;5;241m.\u001B[39msplit(subjects, groups\u001B[38;5;241m=\u001B[39msubjects):\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# print(\"%s %s\" % (subjects[train_split], subjects[test_split]))\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     X_train_features_cleaned \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan_to_num(np\u001B[38;5;241m.\u001B[39marray(X_features_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m])[train_split], nan\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, posinf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, neginf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10000\u001B[39m)\n\u001B[1;32m     31\u001B[0m     X_test_features_cleaned \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan_to_num(np\u001B[38;5;241m.\u001B[39marray(X_features_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m])[test_split], nan\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, posinf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, neginf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m10000\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/signal-quality-challenge/lib/python3.8/site-packages/sklearn/model_selection/_split.py:370\u001B[0m, in \u001B[0;36m_BaseKFold.split\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m    368\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(X)\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits \u001B[38;5;241m>\u001B[39m n_samples:\n\u001B[0;32m--> 370\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    371\u001B[0m         (\n\u001B[1;32m    372\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot have number of splits n_splits=\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m greater\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    373\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m than the number of samples: n_samples=\u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    374\u001B[0m         )\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_splits, n_samples)\n\u001B[1;32m    375\u001B[0m     )\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups):\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=0."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T01:13:07.251544Z",
     "start_time": "2024-12-20T01:13:07.231859Z"
    }
   },
   "source": [
    "import sklearn\n",
    "\n",
    "fprs, tprs, thresholds = sklearn.metrics.roc_curve(y_test, test_scores, pos_label=1)\n",
    "fnrs = 1-tprs\n",
    "tnrs = 1-fprs\n",
    "\n",
    "def get_rates(y_test, test_scores, thresholds):\n",
    "    fprs_ = []\n",
    "    tprs_ = []\n",
    "    fnrs_ = []\n",
    "    tnrs_ = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        confusion_matrix = sklearn.metrics.confusion_matrix(y_test, test_scores>=thresh)\n",
    "        print(confusion_matrix.shape)\n",
    "        FP = confusion_matrix[0,1]  \n",
    "        FN = confusion_matrix[1,0]\n",
    "        TP = confusion_matrix[1,1]\n",
    "        TN = confusion_matrix[0,0]\n",
    "        # Sensitivity, hit rate, recall, or true positive rate\n",
    "        tprs_.append(TP/(TP+FN))\n",
    "        # Specificity or true negative rate\n",
    "        tnrs_.append(TN/(TN+FP))  \n",
    "        # Fall out or false positive rate\n",
    "        fprs_.append(FP/(FP+TN))\n",
    "        # False negative rate\n",
    "        fnrs_.append(FN/(TP+FN))\n",
    "        \n",
    "    return np.array(fprs_), np.array(tprs_), np.array(fnrs_), np.array(tnrs_)\n",
    "    \n",
    "fprs2, tprs2, fnrs2, tnrs2 = get_rates(y_test, test_scores, thresholds)\n",
    "\n",
    "print(fprs==fprs2)\n",
    "print(tprs==tprs2)\n",
    "print(fnrs==fnrs2)\n",
    "print(tnrs==tnrs2)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m fprs, tprs, thresholds \u001B[38;5;241m=\u001B[39m sklearn\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mroc_curve(\u001B[43my_test\u001B[49m, test_scores, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      4\u001B[0m fnrs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mtprs\n\u001B[1;32m      5\u001B[0m tnrs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mfprs\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03fdc303e447c85257badfce409906c7c1c4ada5d52025cf8661518429577854"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
